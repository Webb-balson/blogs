{
  
    
        "post0": {
            "title": "Probability Series - Part-1 - Introduction to Probability",
            "content": "Note . We are facing some issues in rendering Latex mathematical equations on this page.Refer opening this blog in Colab (button on the top right) to solve rendering issues. . What is Probability . It is useful to start with defining what probability is. There are three main categories: . 1. Classical probability . $Pr(X=x) = frac{ text{# x outcomes}}{ text{# possible outcomes}} $ . Classical probability is an assessment of possible outcomes of elementary events. Elementary events are assumed to be equally likely . 2. Frequentist probability . $Pr(X=x) = lim_{n rightarrow infty} frac{ text{# times x has occurred}}{ text{# independent and identical trials}} $ . This interpretation considers probability to be the relative frequency &quot;in the long run &quot; of outcomes. . 3. Subjective probability . Subjective probability is a measure of one&#39;s uncertainty in the value of (X ). It characterizes the state of knowledge regarding some unknown quantity using probability.In simple terms,An event&#39;s probability is the degree of belief that the event will occur. Subjective probabilities are personal judgments based on all of the assessor&#39;s previous experience relevant to the situation at hand. . It is not associated with long-term frequencies nor with equal-probability events. For example: . X = the true prevalence of diabetes in Bengaluru is &lt; 15, | X = the blood type of the person sitting next to you is type A | X = it is raining in Bengaluru,India | . We will cover on this more in our future blogs where we delve into Frequentist vs Bayesian Approach to Statistics but for now we will consider Frequentist approach and will talk about Probabilites as frequency. . General Defintion which I like: . Probability is the measure of certainity of an event taking place. The world we see around us is full of phenomena we perceive as random or unpredictable. We aim to model this phenomena as outcomes of some experiment. The outcomes are a result of sample space and subsets of sample space are called events. The events will be assigned a probability a number between 0 and 1 that expresses how likely the event is to occur. . Elements of Probabilty . Sample Space(Ω) | Sets whose elements describe the outcomes of the experiment in which we are interested.In simpler terms ,it is the set of all outcomes in an experiment. In the case of an experiment on the month,a person&#39;s birthday falls, an obvious choice for the sample space is . Ω = {Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec} . Events(F) | Subsets of the sample space are called events.We say that an event A occurs if the outcome of the experiment is an element of the set A. For eg. In the birthday experiment we can ask for the outcomes that correspond to a long month, i.e., a month with 31 days. . This is the event L = {Jan, Mar, May, Jul, Aug, Oct, Dec}. . Lets define an another event R - that corresponds to the months that have the letter r in their (full) name (so R = {Jan, Feb, Mar, Apr, Sep, Oct, Nov, Dec} . 1. Intersection of two events (L ∩ R) i.e if both L and R occur Long months that contain the letter r are : {Jan, Mar, Oct, Dec} 2. Union of two events (L U R) i.e atleast one of the events L and R occurs. 3. Disjoint or Mutually Exclusive (L ∩ R) = ∅ i.e L and R have no outcomes in common i.e they cannot occur at the same time. . Probability Measure(P) | To express how likely it is that an event occurs, we will assign a probability to each event.Since each event has to be assigned a probability, we speak about Probability function. . This probabilty function has to satisfy the following properties: If we define a probability function P on a finite sample space Ω assigns to each event A in Ω denoting P(A) as Probability that A occurs then: . P(A)&gt;= 0 for all A belonging to F | P(Ω) = 1 i.e outcome of the experiment is always an element of the sample space | P(A U B) = P(A) + P(B) if A and B are disjoint | OR and AND Operations Given two events A and B ,that are not disjoint: . $P(A or B) = P(A) + P(B) - P(A and B)$ or can be denoted as $ P(A U B) = P(A) + P(B) - P(A ∩ B) $ . Products of sample spaces . Suppose we throw a coin two times. What is the sample space associated with this new experiment? It is clear that it should be the set Ω = {H, T } × {H, T } = {(H, H), (H, T ), (T, H), (T, T )} . General Notation Two experiments with sample spaces Ω1 and Ω2 then the combined experiment has as its sample space the set Ω = Ω 1 × Ω 2. If Ω 1 has r elements and Ω 2 has s elements, then Ω 1 × Ω 2 has rs elements . Conditional Probabilty and Independence . Knowing that an event has occurred sometimes forces us to reassess the probability of another event; the new probability is the conditional probability. . Lets pick up our previous example: Birthday Months Ω = {Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec} Born in a long month - L = {Jan, Mar, May, Jul, Aug, Oct, Dec} Born in a month with the letter &#39;r&#39; - R ={Jan, Feb, Mar, Apr, Sep, Oct, Nov, Dec} . Hence, P(L) = $7/12$ and P(R) = $8/12$ . No if we get to know that a person was born in a long month and wanted to find out whether he was born in a month with the letter &#39;r&#39; . In this case , we will filter out some of the outcomes of a sample space belonging to R which is not possible now.It cannot be February, April, June, September, or November as thay are not in our long months set. . So our answer would be out of 7 possible outcomes in R , now only 4 is possible. We call this the conditional probability of R given L i.e P(R/L) = $4/7$ . Note . P(R/L) is not same as P(R ∩ L) as P(R ∩ L) = $4/12$ . A good way to get an intuition on this conept would be to think like P(R | L) is the proportion that P(R ∩ L) is of P(L) or in simple terms it means finding which fraction of the probability of L is also in the event R . For better intuition and visualisation refer the below tree diagram tounderstand the concept . . The conditional probability of R given L is given by: . $P(R | L)$ = $ frac{P(R ∩ L)}{P(L)} $ provided P(L) &gt; 0 . where: . $P(R | L)$ - Conditional Probability . $P(R ∩ L)$ - Joint Probability . $P(L)$ - Marginal Probability . Multiplication Rule . Using the above derived rule of Conditional Probability and multiplying both sides by P(L) . $P(R ∩ L) = P(R | L) . P(L) $ . Computing the probability of R ∩ L can hence be decomposed into two parts, computing P(L) and P(R | L) separately.Here we are conditioning on L. . Note Conditioning can help to make computations easier, but it matters how it is applied. To compute P(R ∩ L) we may condition on L to get $P(R ∩ L) = P(R | L) · P(L)$ or we may condition on R and get $P(R ∩ L) = P(L | R) · P(R)$. Both ways are valid, but often one of $P(R | L)$ and $P(L | A)$ is easy and the other is not. . Let us take an example and validate our argument. Suppose we meet two arbitrarily chosen people. What is the probability their birthdays i.e P(B2) are different? . Whatever the birthday of the first person is, there is only one day the second person cannot pick as birthday, so: $P(B_{2}) = 1 − frac{1}{365} $ . Simple right. But what if we extend this problem to 3 person or say n numbers? This is where Conditional Probabilty becomes handy. . Lets take an example with 3 persons Probability that 3 persons birthdays i.e P(B3) are different The event B3 can be seen as the intersection of the event B2 i.e. “the first two have different birthdays,” with event A3 i.e “the third person has a birthday that does not coincide with that of one of the first two persons.” . or $P(B_3 ) = P(A_3 ∩ B_2 ) = P(A_3 | B_2 )P(B_2 )$ . where $P(A_3 | B_2 ) = (1 − frac{2}{365}) $ and . and $P(B_2) = (1- frac{1}{365}) $ . Hence, $P(B_3) = (1 − frac{2}{365}). (1- frac{1}{365}) $ . So you can see , we now have a framwork which can be applied for n numbers as well. . $P(B_n) = (1- frac{n-1}{365})..(1 − frac{2}{365}). (1- frac{1}{365}) $ . This same problem can be solved if we try conditioning on $A_3$ like . $P(B_3 ) = P(A_3 ∩ B_2 ) = P(B_2 | A_3 )P(A_3 )$ but just trying to understand the conditional probability P(B 2 | A 3 ) already is confusing: The probability that the first two persons’ birthdays differ given that the third person’s birthday does not coincide with the birthday of one of the first two . . . ? . Takeaway Conditioning should lead to easier probabilities; if not, it is probably the wrong approach . Law of Total Probability and Bayes&#39; Rule . Lets start with an example again . Problem Statement We have the following data: An infected patient has a 70% chance of testing positive and a healthy patient just 10% chance of testing positive for the disease.Probability of having a diesase is 0.02.Determine the Probability of an arbitrary patient testing positive to the disease? . P(Testing Positive(T) given the disease(D)) or $P(T|D) = 0.7 $ . P(Testing Positive(T) given no disease(Dc)) or $P(T|D^c)) = 0.1 $ . P(Getting a Disease(D)) or $P(D) = 0.02$ . P(Testing Positive) or $P(T) = ?$ . Solution Patient testing positive is either infected or it is not. So all in all we have these two scearios. . $T = (T ∩ D) ∪ (T ∩ D^c )$ i.e $P(T ) = P(T ∩ D) + P(T ∩ D^c )$ . and we know from Condtionality . $P(T ∩ D) = P(T | D) · P(D)$ . $P(T ∩ D^c ) = P(T | D^c ) · P(D^c )$ . Putting the values in the equation: . $P(T ) = 0.02 · 0.70 + (1 − 0.02) · 0.10 = 0.112$ . This is an application of the law of total probability: computing a probability through conditioning on several disjoint events that make up the whole sample space (in this case two). . If we generalize this.. . The law of total Probability . Suppose $C_1 , C_2 , . . . , C_m$ are disjoint events such that $C_1 ∪ C_2 ∪ · · · ∪ C_m = Ω.$ The probability of an arbitrary event A can be expressed as: . $P(A) = P(A | C_1 )P(C_1 ) + P(A | C_2 )P(C_2 ) + · · · + P(A | C_m )P(C_m )$ . Bayes&#8217; rule. . Another follow up question we can ask in the above problem statement is.. If a patient tests positive; what is the probability it really has the disease i.e $P(D|T)$ ? . Solution . $P(D|T) = frac{P(D ∩ T)}{P(T)} or frac{P(T ∩ D)}{P(T)} $ . $P(D|T) = frac{P(T | D) · P(D)}{P(T | D) · P(D) + P(T | D^c ) · P(D^c )} $ = 0.125 . Similary we can do for $P(D|T_c)$ which will be equal to 0.0068 . What we have just seen is known as Bayes’ rule. . . Suppose the events C_1 , C_2 , . . . , C_m are disjoint and C_1 ∪ C_2 ∪ · · · ∪ C_m = Ω. The conditional probability of C_i , given an arbitrary event A, can be expressed as: . $P(C_i | A) = frac{P(A | C_i ) · P(C_i )}{P(A | C_1 )P(C_1 ) + P(A | C_2 )P(C_2 ) + · · · + P(A | C_m )P(C_m )} $ . Independence . Consider three probabilities from the previous section: . $P(B) = 0.02$ i.e If we know nothing about a person, we would say that there is a 2% chance it is infected . $P(B | T ) = 0.125$ i.e if we know it tested positive, we can say there is a 12.5% chance the person is infected . $P(B | T_c ) = 0.0068.$ if it tested negative, there is only a 0.68% chance . We see that the two events are related in some way: the probability of D depends on whether T occurs. . Imagine the opposite: the test is useless. . Whether the person is infected is unrelated to the outcome of the test, and knowing the outcome of the test does not change our probability of D: $P(D | T ) = P(D)$. In this case we would call D independent of T . Defintion An event A is called independent of B if $P(A | B) = P(A)$ . Let us see how our earlier formula changes in the light of Independence . $P(A ∩ B) = P(A | B)P(B) = P(A) P(B)$ . $P(B | A) = frac{P(A ∩ B)}{P(A)} = frac{P(A).P(B)}{P(A)} $ = P(B) . To show that A and B are independent it suffices to prove just one of the following: . $P(A | B) = P(A)$ | $P(B | A) = P(B)$ | $P(A ∩ B) = P(A) P(B)$ | If one of these statements holds, all of them are true. If two events are not independent, they are called dependent. . Recall the example of birthday events L “born in a long month” and R “born in a month with the letter r.” Let H be the event “born in the first half of the year,” so $P(H) = 1/2.$ Also, $P(H | R) = 1/2$. So H and R are independent. . Extending it to two or more events . Independence of two or more events. Events $A_1 , A_2 , . . . , A_m$ are called independent if: . $P(A_1 ∩ A_2 ∩ · · · ∩ A_m ) = P(A_1 ) P(A_2 ) · · · P(A_m )$ . Note - Independence for three events A, B, and C is not the same as: A and B are independent; B and C are independent; and A and C are independent. . Marginal Probablity in Matrix Form . Lets take an example and see how does it fit in a matrix form . Let $P(X_1 = 0) = 3/4$ and $P(X_1 = 1) = 1/4$ . Given $P(X_2 = 1 | X_1= 0) = 1/3$ $P(X_2 = 1 | X_1= 1) = 0$. Then $P(X_2 = 0 | X_1 = 0) = 2/3$ , $P(X_2 = 0 | X_1 = 1) = 1$ . In Matrix Form . $X_1$ : begin{bmatrix} 3/4 &amp; 1/4 end{bmatrix} . $X_1X_2$ : where $X_1$ is the row and $X_2$ is the column in the matrix with row and column headers as [ 0,1 ] begin{bmatrix} 2/3 &amp; 1/3 0 &amp; 1 end{bmatrix} . We can obtain the expression for $P(X_2)$ easily using matrix notation . which is $P(X_2) = $$ sum$$P(X_1 )P(X_2 | X_1 )$ . $ X_2$ : begin{bmatrix} 3/4 &amp; 1/4 end{bmatrix} . Test Yourself . Explain Probability in your terms? | What are Axioms of Probability? | Define Sample Space and Events? | How does a sample space change when we combine multiple experiments as one experiment? | Intuition on Conditional Probability? | Expression of Conditional Probability? | Difference between Conditional Probability vs Joint Probability and Marginal Probability? | Understanding different expressions like : $P(A | B) , P(A ∩ B),P(A U B), P(A), P(AB)$ | Intuition on Baye&#39;s rule ? | Intuition on Independence Condition and changes in the follwing expressions in case of independence $P(A ∩ B), P(B | A), P(A | B)$ ? | Questions . If we perform an experiment with outcomes 1 (success) and 0 (failure) five times, and we consider the event A “exactly one experiment was a success,.Find P(A) given P(Getting outcome 1) = p . Answer $5(1 − p)^4 p$ . | What is the probability of the event B “exactly two experiments were successful”? . Answer $10(1 − p)^3 p^2$ . | Suppose an experiment in a laboratory is repeated every day of the week until it is successful, the probability of success being p. The first experiment is started on a Monday. What is the probability that the series ends on the next Sunday. . Answer $p(1 − p)^6$ . |",
            "url": "https://entiretydotai.github.io/blogs/jupyter/probability/bayesrule/conditionalprobability/2020/08/08/Probability-Series-Part-1-Introduction-to-Probability.html",
            "relUrl": "/jupyter/probability/bayesrule/conditionalprobability/2020/08/08/Probability-Series-Part-1-Introduction-to-Probability.html",
            "date": " • Aug 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Convolutions in Deep Neural Network",
            "content": "Why CNN and not Regular Neural Nets . 1. Regular Neural Nets don’t scale well to full images . In MNIST dataset,images are only of size 28x28x1 (28 wide, 28 high, 1 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28x28x1 = 786 weights. This amount still seems manageable, . But what if we move to larger images. . For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200x200x3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting. . 2.Parameter Sharing A feature detector that is useful in one part of the image is probably useful in another part of the image.Thus CNN are good in capturing translation invariance. . Sparsity of connections In each layer,each output value depends only on a small number of inputs.This makes CNN networks easy to train on smaller training datasets and is less prone to overfitting. . 2.3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. . . Convolution . In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other. . However we are interested in understanding the actual convolution operation in the context of neural networks. . An intuitive understanding of Convolution Convolution is an operation done to extract features from the images as these features will be used by the network to learn about a particular image.In the case of a dog image,the feature could be the shape of a nose or the shape of an eye which will help the network later to identify an image as a dog. . Convolution operation is performed with the help of the following three elements: . 1.Input Image- The image to convolve on . 2.Feature Detector/Kernel/Filter- They are the bunch of numbers in a matrix form intended to extract features from an image.They can be 1dimensional ,2-dimensional or 3-dimensional . 3.Feature Map/Activation Map- The resultant of the convolution operation performed between image and feature detector gives a Feature Map. . . . . Convolution Operation . . Another way to look at it . . Let say we have an input of $6 x 6$ and a filter size $3 x 3$ . Feature map is of size $4 x 4$ . Convolution over Volume . What if our input image has more than one channel? . Let say we have an input of $6 x 6 x 3$ and a filter size $3 x 3 x 3$ . Feature map is of size $4 x 4$ . . Convolution Operation with Multiple Filters . Let say we have an input of $6 x 6 x 3$ and we use two filters size $3 x 3$ . Feature map is of size $4 x 4 x 2$ . . General Representation . $$Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c&#39;)--Feature Map--[(n-f+1)*(n-f+1)*n(c&#39;)]$$ . $n(h)$-height of the image . $n(w)$-width of the image . $n(c)$-number of channel in an image . $f(h)$-height of the filter . $f(w)$-width of the filter . $f(c&#39;)$-no of the filter . One Convolution layer . Strides . Padding . Pooling . Why we do Pooling? . The idea of max pooling is: . to reduce the size of representation in such a way that we carry along those features which speaks louder in the image | to lower the number of parameters to be computed,speeding the computation | to make some of the features that detects significant things a bit more robust. | Analogy that I like to draw . A good analogy to draw here would be to look into the history of shapes of pyramid. . The Greek pyramid is the one without max pooling whereas the Mesopotamian pyramid is with max pooling involved where we are loosing more information but making our network simpler than the other one. . But don&#39;t we end up loosing information by max pooling? . Yes we do but the question we need to ask is how much information we can afford to loose without impacting much on the model prediction. . Perhaps the criteria to choose how often(after how many convolutions) and at what part of the network (at the beginning or at the mid or at the end of the network) to use max pooling depends completely on what this network is being used for. . For eg: . Cats vs Dogs | Identify the age of a person | General Representation-Updated . Including Padding and Stride . $$Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c&#39;)--Feature Map--[((n-f+2p)/s+1)*((n-f+2p)/s+1)*n(c&#39;)]$$ . $n(h)$-height of the image . $n(w)$-width of the image . $n(c)$-number of channel in an image . $f(h)$-height of the filter . $f(w)$-width of the filter . $f(c&#39;)$-no of the filter . . Examples . Input volume: 32x32x3 10 5x5 filters with stride 1, pad 2 Output volume size: ? . 32x32x10 . Input volume: 32x32x3 10 5x5 filters with stride 1, pad 2 . Number of parameters in this layer? . 1 x 1 Convolution . At first,the idea of using 1x1 filter seems to not make sense as 1x1 convolution is just multiplying by numbers.We will not be learning any feature here. . But wait... What if we have a layer with dimension 32x32x196,here 196 is the number of channels and we want to do convolution . So 1x1x192 convolution will do the work of dimensionality reduction by looking at each of the 196 different positions and it will do the element wise product and give out one number.Using multiple such filters say 32 will give 32 variations of this number. . Why do we use 1x1 filter . 1x 1 filter can help in shrinking the number of channels or increasing the number of channels without changing the height and width of the layer. . | It adds nonlinearity in the network which is useful in some of the architectures like Inception network. . | . Receptive Field . The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by). A receptive field of a feature can be described by its center location and its size. . . Things to remember . Filter will always have the same number of channel as the image. . | Convolving gives a 2 D feature map although our image and kernel used are of 3 dimensional . | Padding -Preserves the feature size . | Pooling Operation- Reduces the Input feature size by half . | References . Convolution Max Pool Standford Slides Standford Blog An intuitive guide to Convolutional Neural Networks Convolutional Neural Networks Understanding of Convolutional Neural Network Receptive Feild Calculation Understanding Convolution in Deep Learning Visualize Image Kernel Visualizing and Understanding Convolution Networks .",
            "url": "https://entiretydotai.github.io/blogs/deep%20neural%20network/convolutions/computer-vision/2020/07/06/Convolutions.html",
            "relUrl": "/deep%20neural%20network/convolutions/computer-vision/2020/07/06/Convolutions.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "CNN Architectures(LeNet to DenseNet)",
            "content": "Introduction . In this post,we will talk about some of the most important papers that have been published over the last 5 years and discuss why they’re so important.We will go through different CNN Architectures (LeNet to DenseNet) showcasing the advancements in general network architecture that made these architectures top the ILSVRC results. . What is ImageNet . ImageNet . ImageNet is formally a project aimed at (manually) labeling and categorizing images into almost 22,000 separate object categories for the purpose of computer vision research. . However, when we hear the term “ImageNet” in the context of deep learning and Convolutional Neural Networks, we are likely referring to the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC for short. . The ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes. . The goal of this image classification challenge is to train a model that can correctly classify an input image into 1,000 separate object categories. . Models are trained on ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing. . These 1,000 image categories represent object classes that we encounter in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types, and much more. You can find the full list of object categories in the ILSVRC challenge . When it comes to image classification, the ImageNet challenge is the de facto benchmark for computer vision classification algorithms — and the leaderboard for this challenge has been dominated by Convolutional Neural Networks and deep learning techniques since 2012. . LeNet-5(1998) . Gradient Based Learning Applied to Document Recognition . A pioneering 7-level convolutional network by LeCun that classifies digits, | Found its application by several banks to recognise hand-written numbers on checks (cheques) | These numbers were digitized in 32x32 pixel greyscale which acted as an input images. | The ability to process higher resolution images requires larger and more convolutional layers, so this technique is constrained by the availability of computing resources. | . AlexNet(2012) . ImageNet Classification with Deep Convolutional Networks . One of the most influential publications in the field by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that started the revolution of CNN in Computer Vision.This was the first time a model performed so well on a historically difficult ImageNet dataset. | The network consisted 11x11, 5x5,3x3, convolutions and made up of 5 conv layers, max-pooling layers, dropout layers, and 3 fully connected layers. | Used ReLU for the nonlinearity functions (Found to decrease training time as ReLUs are several times faster than the conventional tanh function) and used SGD with momentum for training. | Used data augmentation techniques that consisted of image translations, horizontal reflections, and patch extractions. | Implemented dropout layers in order to combat the problem of overfitting to the training data. | Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay. | AlexNet was trained for 6 days simultaneously on two Nvidia Geforce GTX 580 GPUs which is the reason for why their network is split into two pipelines. | AlexNet significantly outperformed all the prior competitors and won the challenge by reducing the top-5 error from 26% to 15.3% | ZFNet(2013) . Visualizing and Understanding Convolutional Neural Networks This architecture was more of a fine tuning to the previous AlexNet structure by tweaking the hyper-parameters of AlexNet while maintaining the same structure but still developed some very keys ideas about improving performance.Few minor modifications done were the following: . AlexNet trained on 15 million images, while ZF Net trained on only 1.3 million images. | Instead of using 11x11 sized filters in the first layer (which is what AlexNet implemented), ZF Net used filters of size 7x7 and a decreased stride value. The reasoning behind this modification is that a smaller filter size in the first conv layer helps retain a lot of original pixel information in the input volume. A filtering of size 11x11 proved to be skipping a lot of relevant information, especially as this is the first conv layer. | As the network grows, we also see a rise in the number of filters used. | Used ReLUs for their activation functions, cross-entropy loss for the error function, and trained using batch stochastic gradient descent. | Trained on a GTX 580 GPU for twelve days. | Developed a visualization technique named Deconvolutional Network, which helps to examine different feature activations and their relation to the input space. Called deconvnet because it maps features to pixels (the opposite of what a convolutional layer does). | It achieved a top-5 error rate of 14.8% | VggNet(2014) . VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION . This architecture is well konwn for Simplicity and depth.. VGGNet is very appealing because of its very uniform architecture.They proposed 6 different variations of VggNet however 16 layer with all 3x3 convolution produced the best result. . Few things to note: . The use of only 3x3 sized filters is quite different from AlexNet’s 11x11 filters in the first layer and ZF Net’s 7x7 filters. The authors’ reasoning is that the combination of two 3x3 conv layers has an effective receptive field of 5x5. This in turn simulates a larger filter while keeping the benefits of smaller filter sizes. One of the benefits is a decrease in the number of parameters. Also, with two conv layers, we’re able to use two ReLU layers instead of one. | 3 conv layers back to back have an effective receptive field of 7x7. | As the spatial size of the input volumes at each layer decrease (result of the conv and pool layers), the depth of the volumes increase due to the increased number of filters as you go down the network. | Interesting to notice that the number of filters doubles after each maxpool layer. This reinforces the idea of shrinking spatial dimensions, but growing depth. | Worked well on both image classification and localization tasks. The authors used a form of localization as regression (see page 10 of the paper for all details). | Built model with the Caffe toolbox. | Used scale jittering as one data augmentation technique during training. | Used ReLU layers after each conv layer and trained with batch gradient descent. | Trained on 4 Nvidia Titan Black GPUs for two to three weeks. | It achieved a top-5 error rate of 7.3% | . In Standard ConvNet, input image goes through multiple convolution and obtain high-level features. . After Inception V1 ,the author proposed a number of upgrades which increased the accuracy and reduced the computational complexity.This lead to many new upgrades resulting in different versions of Inception Network : . Inception v2 | Inception V3 | Inception Network (GoogleNet)(2014) . Going Deeper with Convolutions . Prior to this, most popular CNNs just stacked convolution layers deeper and deeper, hoping to get better performance,however Inception Network was one of the first CNN architectures that really strayed from the general approach of simply stacking conv and pooling layers on top of each other in a sequential structure and came up with the Inception Module.The Inception network was complex. It used a lot of tricks to push performance; both in terms of speed and accuracy. Its constant evolution lead to the creation of several versions of the network. The popular versions are as follows: . Inception v1. | Inception v2 and Inception v3. | Inception v4 and Inception-ResNet. | Each version is an iterative improvement over the previous one.Let us go ahead and explore them one by one . Inception V1 . Inception v1 . Problems this network tried to solve: . What is the right kernel size for convolution A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally. Ans- Filters with multiple sizes.The network essentially would get a bit “wider” rather than “deeper” | How to stack convolution which can be less computationally expensive Stacking them naively computationally expensive. Ans-Limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions | How to avoid overfitting in a very deep network Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network. Ans-Introduce two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. | The total loss used by the inception net during training. total_loss = real_loss + 0.3 aux_loss_1 + 0.3 aux_loss_2 . . Points to note . Used 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep… | No use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters. | Uses 12x fewer parameters than AlexNet. | Trained on “a few high-end GPUs within a week”. | It achieved a top-5 error rate of 6.67% | Inception V2 . Rethinking the Inception Architecture for Computer Vision . Upgrades were targeted towards: . Reducing representational bottleneck by replacing 5x5 convolution to two 3x3 convolution operations which further improves computational speed The intuition was that, neural networks perform better when convolutions didn’t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a “representational bottleneck” | Using smart factorization method where they factorize convolutions of filter size nxn to a combination of 1xn and nx1 convolutions. For example, a 3x3 convolution is equivalent to first performing a 1x3 convolution, and then performing a 3x1 convolution on its output. They found this method to be 33% more cheaper than the single 3x3 convolution. | ResNet(2015) . Deep Residual Learning for Image Recognition In ResNet, identity mapping is proposed to promote the gradient propagation. Element-wise addition is used. It can be viewed as algorithms with a state passed from one ResNet module to another one. . . ResNet-Wide . left: a building block of [2], right: a building block of ResNeXt with cardinality = 32 . DenseNet(2017) . Densely Connected Convolutional Networks It is a logical extension to ResNet. . From the paper: Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. . DenseNet Architecture . Let us explore different componenets of the network 1. Dense Block Feature map sizes are the same within the dense block so that they can be concatenated together easily. . In DenseNet, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers. . Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e. number of channels can be fewer. The growth rate k is the additional number of channels for each layer. . The paper proposed different ways to implement DenseNet with/without B/C by adding some variations in the Dense block to further reduce the complexity,size and to bring more compression in the architecture. . 1. Dense Block (DenseNet) - Batch Norm (BN) - ReLU - 3×3 Convolution 2. Dense Block(DenseNet B) - Batch Norm (BN) - ReLU - 1×1 Convolution - Batch Norm (BN) - ReLU - 3×3 Convolution 3. Dense Block(DenseNet C) - If a dense block contains m feature-maps, The transition layer generate $ theta $ output feature maps, where $ theta leq theata leq$ is referred to as the compression factor. - $ theta$=0.5 was used in the experiemnt which reduced the number of feature maps by 50%. 4. Dense Block(DenseNet BC) - Combination of Densenet B and Densenet C . 2. Trasition Layer The layers between two adjacent blocks are referred to as transition layers where the following operations are done to change feature-map sizes: . - 1×1 Convolution - 2×2 Average pooling . Points to Note: . it requires fewer parameters than traditional convolutional networks | Traditional convolutional networks with L layers have L connections — one between each layer and its subsequent layer — our network has L(L+1)/ 2 direct connections. | Improved flow of information and gradients throughout the network, which makes them easy to train | They alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. | Concatenating feature maps instead of summing learned by different layers increases variation in the input of subsequent layers and improves efficiency. This constitutes a major difference between DenseNets and ResNets. | It achieved a top-5 error rate of 6.66% | MobileNet . Spatial Seperable Convolution . Divides a kernel into two, smaller kernels . . Instead of doing one convolution with 9 multiplications(parameters), we do two convolutions with 3 multiplications(parameters) each (6 in total) to achieve the same effect . . With less multiplications, computational complexity goes down, and the network is able to run faster. . This was used in an architecture called Effnet showing promising results. . The main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels. . Depthwise Convolution . Say we need to increase the number of channels from 16 to 32 using 3x3 kernel. . Normal Convolution Total No of Parameters = 3 x 3 x 16 x 32 = 4608 . . Depthwise Convolution . DepthWise Convolution = 16 x [3 x 3 x 1] | PointWise Convolution = 32 x [1 x 1 x 16] | Total Number of Parameters = 656 . Mobile net uses depthwise seperable convolution to reduce the number of parameters . References . Standford CS231n Lecture Notes The 9 Deep Learning Papers You Need To Know About CNN Architectures Lets Keep It Simple CNN Architectures Keras Inception Versions DenseNet Review DenseNet ResNet ResNet Versions Depthwise Convolution .",
            "url": "https://entiretydotai.github.io/blogs/deep%20neural%20network/convolutions/computer-vision/imagene/lenet/alexnet/zfnet/vgg/inception-v1/inception-v2/inception-v3/googlenet/resnet/resnet-wide/densenet/mobilenet/spatial-separable-convolutions/depthwise-convolutions/2020/07/06/CNN-Architectures.html",
            "relUrl": "/deep%20neural%20network/convolutions/computer-vision/imagene/lenet/alexnet/zfnet/vgg/inception-v1/inception-v2/inception-v3/googlenet/resnet/resnet-wide/densenet/mobilenet/spatial-separable-convolutions/depthwise-convolutions/2020/07/06/CNN-Architectures.html",
            "date": " • Jul 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Time and Memory Profiling for Python",
            "content": "Introduction . Every Developer desires to make their code optimzied and efficient. A dream where the developers want that their code to execute faster, with no memory leakage on the production system. . Let&#39;s make this dream true... . Creating Data-processing pipeline, writing new algorithms, Deploying Machine Learning models to server million users, Scientific calculation in astropyhsics, these are few areas where when we write code we want to profile every single line of code for two things . The amount of time it is taking to execute where our goal is to reduce the time taken a.k.a Time Complexity. | The memory consumption for execution of that code where our goal is to reduce the memory usage a.k.a Space complexity. | There always a trade-off between both of them some time we are fine with memory consumption but not with the time it takes and vice-versa based on the needs we check for the trade-off, but the best system is where we can reduce both space and time complexity. . Premature Optimization is evil. . Early in developing Algorithms we should think less about these things because it can be counter-productive which can lead to premature optimization and its the root cause of all evil. . So first make it work then optimize it. . Magic functions and tools . While most of the data science experiments starts in Ipython Notebook. The Ipython enviroment gives us some magic functions which can be utilized to profile our code. . %%timeit: Measuring time taken for the codeblock to run | %lprun: Run code with the line-by-line profiler | %mprun: Run code with the line-by-line memory profiler | For Tracing Memory Leakage we can use Pympler. . import numpy as np . Timeit . The usage of timeit is very simple just put the magic method on the top of the cell and it will calculate the time taken to execute the cell. . Let&#39;s compare vectorized vs non-vectorized version of numpy code. . number = np.random.randint(0,100,10000) . %%timeit total = 0 for i in number: total+=i . 3.11 ms ± 86.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . %%timeit number.sum() . 14.9 µs ± 74.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . The difference in the execution time is evident one. . Non-vectorized code in Milliseconds, 10-3. Vectorized code in Microseconds, 10-6. . Vectorized code is the winner here. . Timing Profiling with Lprun . line_profiler is a package for doing line-by-line timing profiling of functions. . Install using . pip install line_profiler . Python provides a builtin profiler, but we will be using Line profiler for reasons stated below. . The current profiling tools supported in Python 2.7 and later only time function calls. This is a good first step for locating hotspots in one&#39;s program and is frequently all one needs to do to optimize the program. However, sometimes the cause of the hotspot is actually a single line in the function, and that line may not be obvious from just reading the source code. These cases are particularly frequent in scientific computing. Functions tend to be larger (sometimes because of legitimate algorithmic complexity, sometimes because the programmer is still trying to write FORTRAN code), and a single statement without function calls can trigger lots of computation when using libraries like numpy. cProfile only times explicit function calls, not special methods called because of syntax. Consequently, a relatively slow numpy operation on large arrays like this, . a[large_index_array] = some_other_large_array . is a hotspot that never gets broken out by cProfile because there is no explicit function call in that statement. . LineProfiler can be given functions to profile, and it will time the execution of each individual line inside those functions. In a typical workflow, one only cares about line timings of a few functions because wading through the results of timing every single line of code would be overwhelming. However, LineProfiler does need to be explicitly told what functions to profile. . # once installed we have load the extension %load_ext line_profiler . def some_operation(x): x = x **2 x = x +2 x = np.concatenate([x,x,x],axis=0) return x . Now the %lprun command will do a line-by-line profiling of any function–in this case, we need to tell it explicitly which functions we&#39;re interested in profiling: . %lprun -f some_operation some_operation(np.random.randn(100)) . Timer unit: 1e-06 s Total time: 7.7e-05 s File: &lt;ipython-input-30-80aca4fcfa96&gt; Function: some_operation at line 1 Line # Hits Time Per Hit % Time Line Contents ============================================================== 1 def some_operation(x): 2 1 24.0 24.0 31.2 x = x **2 3 1 22.0 22.0 28.6 x = x +2 4 1 30.0 30.0 39.0 x = np.concatenate([x,x,x],axis=0) 5 1 1.0 1.0 1.3 return x . The source code of the function is printed with the timing information for each line. There are six columns of information. . Line : The line number in the file. | Hits: The number of times that line was executed. | Time: The total amount of time spent executing the line in the timer&#39;s units. In the header information before the tables, you will see a line &quot;Timer unit:&quot; giving the conversion factor to seconds. It may be different on different systems. | Per Hit: The average amount of time spent executing the line once in the timer&#39;s units. | % Time: The percentage of time spent on that line relative to the total amount of recorded time spent in the function. | Line Contents: The actual source code. Note that this is always read from disk when the formatted results are viewed, not when the code was executed. If you have edited the file in the meantime, the lines will not match up, and the formatter may not even be able to locate the function for display. | . Memory Profiling with mprun . This is a python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs. . Install . pip install -U memory_profiler . The only issue mprun doesn&#39;t work on notebook rather on a python file so we will write the code in notebook %%file magic function we will write that into a file and execute mprun on it . %load_ext memory_profiler . %%file mprun.py import numpy as np def some_operation(x): y = x **2 z = y +2 result = np.concatenate([x,y,z],axis=0) return result . Overwriting mprun.py . from mprun import some_operation %mprun -f some_operation some_operation(np.random.randn(100000)) . . Line # Mem usage Increment Line Contents ================================================ 2 62.5 MiB 62.5 MiB def some_operation(x): 3 63.3 MiB 0.8 MiB y = x **2 4 64.0 MiB 0.8 MiB z = y +2 5 66.3 MiB 2.3 MiB result = np.concatenate([x,y,z],axis=0) 6 66.3 MiB 0.0 MiB return result . The first column represents the line number of the code that has been profiled. | The second column (Mem usage) the memory usage of the Python interpreter after that line has been executed. | The third column (Increment) represents the difference in memory of the current line with respect to the last one. | The last column (Line Contents) prints the code that has been profiled. | . Memory Leakage using pympler . Pympler is a development tool to measure, monitor and analyze the memory behavior of Python objects in a running Python application. . By pympling a Python application, detailed insight in the size and the lifetime of Python objects can be obtained. Undesirable or unexpected runtime behavior like memory bloat and other “pymples” can easily be identified. . Pympler integrates three previously separate modules into a single, comprehensive profiling tool. The asizeof module provides basic size information for one or several Python objects, module muppy is used for on-line monitoring of a Python application and module Class Tracker provides off-line analysis of the lifetime of selected Python objects. . A web profiling frontend exposes process statistics, garbage visualisation and class tracker statistics. . Hit table of content for tutorial . Read More . Understanding Python Memory Managment . Python Garbage Collector .",
            "url": "https://entiretydotai.github.io/blogs/profiling/timing/memory/memoryleakage/profiler/memory_profiler/line_profiler/optimize/2020/06/01/profiling.html",
            "relUrl": "/profiling/timing/memory/memoryleakage/profiler/memory_profiler/line_profiler/optimize/2020/06/01/profiling.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Metrics for Machine Learning",
            "content": "Metrics . Example . Let’s take an example of Binary classification where the task is to predict whether we have Dog or not. So in an Image if model predicts Dog that’s a positive class if it predicts no Dog that’s a negative class. . Confusion Matrix . A confusion matrix is a table that is used to describe the performance of a classification model. . . Let’s consider this example table where N denotes the total number of images our model will predict upon. . N = 150 Total Number of Dog Images = 100 Total Number of No Dog Images = 50 . In total 100 Dog Images 1. Model Predicted 60 Correct which is your True Positive. 2. Model Predicted 40 incorrect which is False Negative.(Since It has to predict Dog and It Predicted No Dog which is a False prediction to a Negative class.) . Similarly in 50 No Dog Images 1. Model Predicted 30 Correct which is True Negative. 2. Model Predicted 20 Incorrect which is False Positive. (Since It has to predict No Dog and It predicted Dog which is a False prediction to a Positive class.) . TP-&gt;60 FN-&gt;40 TN-&gt;30 FP-&gt;20 . Accuracy . Accuracy is a basic metric which just tells models overall performance. How many predictions made by model is correct. . Accuracy=TruePositive+TrueNegativeNAccuracy = frac{True Positive + True Negative}{N}Accuracy=NTruePositive+TrueNegative​ . Accuracy=60+30150=0.6=60Accuracy = frac{60+30}{150} = 0.6 = 60Accuracy=15060+30​=0.6=60 . Accuracy gave us a idea about how 60 % prediction was correct. Accuracy can only be a good metric. if all the classes are balanced i.e No of positive sample is approximately equal to No of negative samples. Per class Accuracy can also be calculated to know for which classes model is behaving well. . Misclassification Rate . Misclassification Rate tells overall how poor model performance is. It just opposite of Accuracy. . misclassification rate=1−Accuracymisclassification rate = 1 - Accuracymisclassification rate=1−Accuracy . misclassifcation rate=FalsePostive+FalseNegativeNmisclassifcation rate = frac{False Postive + False Negative}{N}misclassifcation rate=NFalsePostive+FalseNegative​ . misclassification rate=20+40150=0.4=40%misclassification rate = frac{20+40}{150} = 0.4 = 40 %misclassification rate=15020+40​=0.4=40% . Precision / positive predictive value . Precision is another metric which tells while predicting how accurately can I predict positive classes . . Positive class prediction=True positive+False PositivePositive class prediction = True positive + False PositivePositive class prediction=True positive+False Positive . precision=True Positivepositive class predictionprecision = frac{True Positive}{positive class prediction}precision=positive class predictionTrue Positive​ . precision=6060+20=0.75=75%precision = frac {60}{60 + 20} = 0.75 = 75 %precision=60+2060​=0.75=75% . We are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value. . Let’s consider two example where . Spam detection -&gt; Classify whether an email is Spam or Not Spam. . ​ Here the goal is to accurately classify spam emails. It’s okay to classify a Spam mail as Not Spam mail as it will come in our inbox it does no harm to use. But if we classify as Not Spam Mail as Spam mail then there is a problem because we generally do not open our Spam Box. . ​ If you think about it the first case is False Negative and Second case is False Positive and we are okay with False Negative’s but we are not okay with False Positives and our goal is to reduce False Positive. . ​ So in Spam detection task precision is a good metric. since it is inversely proportionate to False Positives. . | Cancer detection -&gt; Classify whether a person has a cancer or not. . ​ Here the goal is to accurately classify whether a person has a cancer or Not. It’s okay to classify a person Not having cancer as cancer. But it’s not okay to predict a person having cancer as Not cancer. . ​ If you think the first case is False Positive and Second case is False Negative and we are okay with False Positive but not okay with False Negative. . ​ Hence in this particular task Precision plays no role. . | Hence to reduce False Positives Precision is used. Precision can easily be effected by class Imbalance. . Negative Predictive Value . Negative Predictive Value is another metric which tells while predicting how accurately can I predict Negative classes . . Negative class prediction=True Negative+False NegativeNegative class prediction = True Negative + False NegativeNegative class prediction=True Negative+False Negative . Negative Prediction Value=True NegativeNegative class predictionNegative Prediction Value = frac{True Negative}{Negative class prediction}Negative Prediction Value=Negative class predictionTrue Negative​ . negative prediction value=6060+20=0.75=75%negative prediction value = frac {60}{60 + 20} = 0.75 = 75 %negative prediction value=60+2060​=0.75=75% . We are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value. . Let Suppose we don’t want to have any additional process for screening patients checked as healthy (not cancer) then we may want to make sure that our negative predictive value is high. . Recall / True Positive Rate / Sensitivity . Recall is another metric which tells us while predicting how accurately can it predict positive classes given a set of Actual Positives. Actual positive class=True Positive+False NegativeActual positive class = True Positive + False NegativeActual positive class=True Positive+False Negative . recall=True PositiveActual positive classrecall = frac{True Positive}{Actual positive class}recall=Actual positive classTrue Positive​ . recall=60100=0.6=60%recall = frac{60}{100} = 0.6 = 60 %recall=10060​=0.6=60% . Here our concern is about Given a set of Positive Samples which is like giving all the Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual positives. That’s why recall is also True Positive Rate . . The reason why it’s called recall is given all the positive sample knowledge how well the model can recall that knowledge to predicted accurately by decreasing the error rate for the Actual positive class. . Going back to the Cancer Example it is very clear know that for Cancer Detection we will use Recall as our metric. Recall is good metric to be used for class Imbalance problem. . Selectivity / True Negative Rate / Specificity . Similar to True positive rate, True Negative rate tells us while predicting how accurately can it predict Negative classes given a set of Actual Negatives. Actual negative class=True Negative+False PositiveActual negative class = True Negative + False PositiveActual negative class=True Negative+False Positive . True Negative Rate=True NegativeActual negative classTrue Negative Rate = frac{True Negative}{Actual negative class}True Negative Rate=Actual negative classTrue Negative​ . True Negative Rate=60100=0.6=60%True Negative Rate = frac{60}{100} = 0.6 = 60 %True Negative Rate=10060​=0.6=60% . Here our concern is about Given a set of Negative Samples which is like giving all the No Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual Negatives. . For the same cancer example True Negative rate will show how many non cancer people are identified as not having cancer. . Sensitivity vs Specificity . . n many tests, including diagnostic medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing “nothing bad” despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive and highly specific does both, so it “rarely overlooks a thing that it is looking for” and it “rarely mistakes anything else for that thing.” Because most medical tests do not have sensitivity and specificity values above 99%, “rarely” does not equate to certainty. But for practical reasons, tests with sensitivity and specificity values above 90% have high credibility, albeit usually no certainty, in differential diagnosis. . Sensitivity, therefore, quantifies the avoidance of false negatives and specificity does the same for false positives. . False Positive Rate / Type I error . When the model predicts something Falsely to the positive class which then it is contributing to the False Positive rate. we can think of it as False alert. For example if in a production house based on certain machine parameters the model has to predict whether the situation insider the production house is dangerous or not and it has to raise alarm if its dangerous. Now if everything is fine and still the model predicts as dangerous situation then that’s a False alarm which you can say a False Positive Rate. . False Positive Rate is just opposite of True Negative Rate Actual negative class=True Negative+False PositiveActual negative class = True Negative + False PositiveActual negative class=True Negative+False Positive . False positive Rate=False PositiveActual negative classFalse positive Rate = frac{False Positive}{Actual negative class}False positive Rate=Actual negative classFalse Positive​ . False Positive Rate=1−True Negative RateFalse Positive Rate = 1 - True Negative RateFalse Positive Rate=1−True Negative Rate . The lower the False Positive Rate the better the model. . False Negative Rate / Type - II error . When the model doesn’t predict something which it should then it is contributing to the False Negative Rate. We can think it as Miss Rate. For example in Online fraud transaction if the model classifies a Fraud Transaction as a Non Fraud Transaction then the model basically missed to catch that Fraud transaction. . False Negative Rate is just of True Positive Rate Actual positive class=True Positive+False NegativeActual positive class = True Positive + False NegativeActual positive class=True Positive+False Negative . False Negative Rate=False NegativeActual positive classFalse Negative Rate = frac{False Negative}{Actual positive class}False Negative Rate=Actual positive classFalse Negative​ . False Negative Rate=1−True Positive RateFalse Negative Rate = 1 - True Positive RateFalse Negative Rate=1−True Positive Rate . False Discovery Rate . False Discovery Rate is just opposite of Precision It measures how many predictions out of all positive predictions were incorrect. Positive class prediction=True positive+False PositivePositive class prediction = True positive + False PositivePositive class prediction=True positive+False Positive . False Discovery Rate=False Positivepositive class predictionFalse Discovery Rate = frac{False Positive}{positive class prediction}False Discovery Rate=positive class predictionFalse Positive​ . False Discovery Rate=1−PrecisionFalse Discovery Rate = 1 - PrecisionFalse Discovery Rate=1−Precision . When raising False alert is expensive it is worth looking every Positive prediction then we should optimize for precision. . False Omission Rate . False Omission Rate is just opposite of Negative Predictive Value False Omission Rate=1−Negative Predictive ValueFalse Omission Rate = 1 - Negative Predictive ValueFalse Omission Rate=1−Negative Predictive Value . F 1 Score (beta = 1 ) . Now that two important metric which is used often is precision and recall and rather then having too look two number F1 score combines precision and recall. . The score lies in the range [0,1] with 1 being ideal and 0 being the worst. The two ways to combine Precision and recall is Arithmetic MeanF1 score=precision+recall2Arithmetic Mean F1 score = frac{precision + recall}{2}Arithmetic MeanF1 score=2precision+recall​ . Harmonic MeanF1 Score=21precision+1recallHarmonic Mean F1 Score = frac{2}{ frac{1}{precision} + frac{1}{recall}}Harmonic MeanF1 Score=precision1​+recall1​2​ . The reason to choose Harmonic mean over Arithmetic mean is precision and recall both have same numerator but different denominators so it makes no sense to average two different things as, fractions are more sensible to average by arithmetic mean when they have the same denominator. Rather we take reciprocal so that the average makes sense. . F 2 Score (beta = 2 ) . It’s a metric that combines precision and recall, putting 2x emphasis on recall. F2 score=1+22precision+1recallF2 score = frac{1+2}{ frac{2}{precision} + frac{1}{recall}}F2 score=precision2​+recall1​1+2​ . F Beta Score . F beta score is a general formula for F1 score and F2 score . When choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us. . With 0&lt;beta&lt;1 we care more about precision Fbeta score=1+ββprecision+1recallF beta score = frac{1+ beta}{ frac{ beta}{precision} + frac{1}{recall}}Fbeta score=precisionβ​+recall1​1+β​ . Averaging parameter . micro . Calculate metrics globally by counting the total number of times each class was correctly predicted and incorrectly predicted. Micro Average captures class-imbalance and will bring down the precision . For example in Iris Dataset the model prediction result is given in the table .   TP FP . Setosa | 45 | 5 | . Virgnica | 10 | 60 | . Versicolor | 40 | 10 | . micro precision=45+10+4045+10+40+5+60+10=0.55micro precision = frac{45 + 10 + 40}{45+ 10 + 40 + 5+60+10 } = 0.55micro precision=45+10+40+5+60+1045+10+40​=0.55 . macro . Calculate metrics for each “class” independently, and find their unweighted mean. This does not take label imbalance into account. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class. Setosa precision=4545+5=0.9virgnica precision=1010+60=0.14versicolor precision=4040+10=0.8Setosa precision = frac{45}{45+5} =0.9 virgnica precision = frac{10}{10 + 60} =0.14 versicolor precision = frac{40}{40+10} = 0.8 Setosa precision=45+545​=0.9virgnica precision=10+6010​=0.14versicolor precision=40+1040​=0.8 . Macro Precision=0.9+0.14+0.83=0.613Macro Precision = frac{0.9+0.14+0.8}{3} = 0.613Macro Precision=30.9+0.14+0.8​=0.613 . weighted accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample. . Precision Recall Curve . Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned. . The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). . A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly. . . ROC-AUC curve . A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate. . The top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better. . The “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate. . It can also be used for Mutli label classification problem. . . Cohen’s kappa . The function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth. . The kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels). . Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators. . For Kappa score formulae and calculation refer Cohen’s kappa . Hamming Loss . The Hamming loss is the fraction of labels that are incorrectly predicted. . Evaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If T denotes the true set of labels for a given sample, and P the predicted set of labels, then the following metrics can be defined on that sample: . Hamming loss: the fraction of the wrong labels to the total number of labels, i.e. . hamming loss=1∣N∣ .∣L∣∑i=1∣N∣∑j=1∣L∣xor(yi,j,zi,j)hamming loss = { frac {1}{|N| . |L|}} sum_{i=1}^{|N|} sum_{j=1}^{|L|}xor (y_{i,j},z_{i,j})hamming loss=∣N∣ .∣L∣1​i=1∑∣N∣​j=1∑∣L∣​xor(yi,j​,zi,j​) . where y_ij target and z_ij is the prediction. This is a loss function, so the optimal value is zero. . Hamming Loss computes Hamming distance and In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming. . Matthews Correlation Coefficient . Till Now For Binary Classification Problem we haven’t encountered any metric which incorporates all 4 parts of the confusion matrix and works good either we have balanced dataset or a Imbalanced one. . Matthews Correlation Coefficient is the answer It is a more reliable statistical rate which produces high score only if the prediction obtained good results in all 4 parts of the confusion matrix. . It computes correlation coefficient between the true class and the predicted class the higher the correlation coefficient the better the model is at prediction. . The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. . MCC=TP×TN−FP×FN(TP+FP)(TP+FN)(TN+FP)(TN+FN)MCC = frac{TP times TN - FP times FN }{ sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}MCC=(TP+FP)(TP+FN)(TN+FP)(TN+FN) . ​TP×TN−FP×FN​ . If there is no False prediction, then the model has +1 as a correlation coefficient since (FP x FN = 0) vice-versa if (TP x TN = 0) then the model has -1 as a correlation coefficient. . Advantages of MCC over accuracy and F1 score . Average Precision Score . AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight: . AP=∑n(Rn−Rn−1)PnAP = sum_n{(R_n-R_{n-1}) P_n}AP=n∑​(Rn​−Rn−1​)Pn​ . where Pn and Rn denotes the nth threshold. . This metric is also used in Object Detection. . Intution Behind Average Precision | Wikipedia Average Precision | Balanced Accuracy . Balanced Accuracy is metric used to deal with Imbalanced dataset. It is the average of Sensitivity and Specificity . In more generic term averaging recall of all classes. . Sensitivity covers the True Positive part and Specificity covers True Negative Part. . Balanced Accuracy=sensitivity+specificity2Balanced Accuracy = frac{sensitivity + specificity}{2}Balanced Accuracy=2sensitivity+specificity​ . Concordance and Discordance . In an ideal model, the probability scores of all true 1’s should be greater than the probability scores of ALL true 0’s. Such a model is said to be perfectly concordant and this phenomenon can be measured by Concordance and Discordance. . So how to calculate Concordance? . Let’s consider the following 4 observation’s actual class and predicted probability scores. . Patient No True Class Probability Score . P1 | 1 | 0.9 | . P2 | 0 | 0.42 | . P3 | 1 | 0.30 | . P4 | 1 | 0.80 | . From the above 4 observations, there are 3 possible pairs of 1’s and 0’s. That is, P1-P2, P3-P2 and P4-P2. . A pair is said to be concordant if the probability score of True 1 is greater than the probability score of True 0. . P1-P2 =&gt; 0.9 &gt; 0.42 =&gt; Concordant! P3-P2 =&gt; 0.3 &lt; 0.42 =&gt; Discordant! P4-P2 =&gt; 0.8 &gt; 0.42 =&gt; Concordant! . Out of the 3 pairs, only 2 are concordant. So, the concordance is 2/3 = 0.66 and discordance is 1 - 0.66 = 0.33. . In simpler words, we take all possible combinations of true events and non-events. Concordance is the percentage of pairs, where true event’s probability scores are greater than the scores of true non-events. . For a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of the model. . References . https://stackoverflow.com/questions/26355942/why-is-the-f-measure-a-harmonic-mean-and-not-an-arithmetic-mean-of-the-precision | https://neptune.ai/blog/evaluation-metrics-binary-classification | https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks | https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin | https://scikit-learn.org/0.22/modules/model_evaluation.html#classification-metrics | https://www.quora.com/How-do-I-interpret-concordance-in-Logistic-Regression# | https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html | https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py |",
            "url": "https://entiretydotai.github.io/blogs/markdown/2020/05/06/metrics-blog.html",
            "relUrl": "/markdown/2020/05/06/metrics-blog.html",
            "date": " • May 6, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Machine Learning Basics",
            "content": "Learning Algorithm . A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean by learning . /bin/sh: 1: Syntax error: word unexpected (expecting &#34;)&#34;) . The Task T . Machine learning tasks are usually described in terms of how the machine learning system should process an example.Many kinds of tasks can be solved with machine learning. Some of the most common machine learning tasks include the following: . Classification | Classification with Missing Inputs | Regression | Transcription | Machine Translation | Structured Output | Anomaly Detection | Synthesis and sampling | Imputation of Missing Value | Denoising | Density Estimation or Probability Mass Function Estimation etc. | . The Performance Measure, P . A quantitative measure to evaluate the abilities of a machine learning algorithm.Usually this performance measure P is specific to the task T being carried out.Some of the common measure includes the following: . Accuracy | Precision | Recall | ROC Curve | F-score etc. | . The Experience, E . Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process. . Supervised learning algorithms: Experience a dataset containing features,but each example is also associated with a label or target. | Unsupervised learning algorithms: Experience a dataset containing many features, then learn useful properties of the structure of this dataset | Reinforcement learning algorithms: Do not just experience a fixed Dataset but also revolves around States,Actions,Environment and Reward | Bias and Variance . Overview . In supervised machine learning an algorithm learns a model from training data. . The goal of any supervised machine learning algorithm is to best estimate the mapping function (f) for the output variable (Y) given the input data (X). The mapping function is often called the target function because it is the function that a given supervised machine learning algorithm aims to approximate. . $Y[pred] = f(x)$ $Y[true]= Y[pred] + Error (e)$ . The prediction error for any machine learning algorithm can be broken down into three parts: . Bias Error | Variance Error | Irreducible Error The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable. | Bias Error Bias are the simplifying assumptions made by a model to make the target function easier to learn. . Generally, parametric algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias. . Low Bias: Suggests less assumptions about the form of the target function. High-Bias: Suggests more assumptions about the form of the target function. Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines. . Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression. . Variance Error Variance is the amount that the estimate of the target function will change if different training data was used. . The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables. . Machine learning algorithms that have a high variance are strongly influenced by the specifics of the training data. This means that the specifics of the training have influences the number and types of parameters used to characterize the mapping function. . Low Variance: Suggests small changes to the estimate of the target function with changes to the training dataset. High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset. Generally, nonparametric machine learning algorithms that have a lot of flexibility have a high variance. For example, decision trees have a high variance . Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression. . Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines. . Bias-Variance Trade-Off . ![](images/Bias_Variance_Tradeoff.png) . The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance. . As seen above . Parametric or linear machine learning algorithms often have a high bias but a low variance. Non-parametric or non-linear machine learning algorithms often have a low bias but a high variance. The parameterization of machine learning algorithms is often a battle to balance out bias and variance. . Underfitting and Overfitting . Before going into Underfitting and Overfitting concept,let us understand what is Train DataValidation DataTest Data . ![](images/Train_Valid_Test_Dataset.png) . When training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the training error, and we try to reduce this training error. . However our goal is not only to achieve minimum training error but also to make generalization error or the test error to be as low as possible . The factors determining how well a machine learning algorithm will perform are its ability to: . Make the training error small. | Make the gap between training and test error small. | The above two factors correspond to the two central challenges in machine learning: Underfitting and Overfitting . Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large. . ![](images/Underfit_Overfit.png) . . ![](images/Bias_Variance_Error_Plot.png) . Capacity . A model’s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set We can control whether a model is more likely to overfit or underfit by altering its capacity One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution. For example,in the above figure the linear regression algorithm has the set of all linear functions of its input as its hypothesis space. We can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model’s capacity. . Hyperparametes and Validation Sets . Most machine learning algorithms have several settings that we can use to control the behavior of the learning algorithm. These settings are called hyperparameters. The values of hyperparameters are not adapted by the learning algorithm itself rather it is a trial and error method done iteratively. . But the question is on which data this model settings aka Hyperparameters needs to be learnt? . What is the problem if hyperparameters are learnt on training data? . If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting. To solve this problem, we need a validation set of examples that the training algorithm does not observe which guide the selection of hyperparameters. Way to go . Construct the validation set from the training data. | Specifically,split the training data into two disjoint subsets. | One of these subsets is used to learn the parameters. | The other subset is our validation set, used to estimate the generalization error during or after training, allowing for the hyperparameters to be updated accordingly. Generally we split the data as 70% train 30% valid or 80% train and 20% valid But what if Data size is too small ?? Cross Validation | Gradient Descent . Introduction . It is an optimization algorithm to find the minimum of a function. We start with a random point on the function and move in the negative direction of the gradient of the function to reach the local/global minima. . Nearly all of deep learning is powered by this very important algorithm with some twist :SGD: . ![](images/Derivatives_GD.png) . The derivative $f&#39;(x)$ gives the slope of $f(x)$ at the point x.In other words, it specifies how to scale a small change in the input in order to obtain the corresponding change in the output. The derivative is therefore useful for minimizing a function because it tells us how to change x in order to make a small improvement in y.We can thus reduce $f(x)$ by moving x in small steps with opposite sign of the derivative. This technique is called gradient descent . Critical Points . When $f&#39;(x) = 0$, the derivative provides no information about which direction to move. Points where $f&#39;(x) = 0$ are known as critical points or stationary points Types of Critical Points: . Local Minimum-Point where $f(x)$ is lower than at all neighboring points, so it is no longer possible to decrease $f(x)$ by making infinitesimal steps. | Local Maximum Point where $f(x)$ is higher than at all neighboring points,so it is not possible to increase $f(x)$ by making infinitesimal steps. | Saddle Points-Some critical points are neither maxima nor minima. | Global Minimum-Point that obtains the absolute lowest value of $f(x)$ | ![](images/Critical_Points.png) . The gradient points directly uphill, and the negative gradient points directly downhill. We can decrease function $f$ by moving in the direction of the negative gradient. This is known as the method of steepest descent or gradient descent. . Example . Question : Find the local minima of the function y=(x+5)² starting from the point x=3 . . Now, let’s see how to obtain the same numerically using gradient descent. . #collapse cur_x = 3 # Tell the algorithm from which point to start.Here we are saying the algorithm to start at x=3 rate = 0.01 # SIze of the step when we move in the direction of the steepest descent (Learning rate) precision = 0.000001 #This tells us when to stop the algorithm previous_step_size = 1 # max_iters = 10000 # maximum number of iterations iters = 0 #iteration counter df = lambda x: 2*(x+5) #Gradient of our function . . #collapse while previous_step_size &gt; precision and iters &lt; max_iters: prev_x = cur_x #Store current x value in prev_x cur_x = cur_x - rate * df(prev_x) #Grad descent previous_step_size = abs(cur_x - prev_x) #Change in x iters = iters+1 #iteration count print(&quot;Iteration&quot;,iters,&quot; nX value is&quot;,cur_x) #Print iterations print(&quot;The local minimum occurs at&quot;, cur_x) . . Iteration 1 X value is 2.84 Iteration 2 X value is 2.6832 Iteration 3 X value is 2.529536 Iteration 4 X value is 2.37894528 Iteration 5 X value is 2.2313663744 Iteration 6 X value is 2.0867390469119997 Iteration 7 X value is 1.9450042659737599 Iteration 8 X value is 1.8061041806542846 Iteration 9 X value is 1.669982097041199 Iteration 10 X value is 1.5365824551003748 Iteration 11 X value is 1.4058508059983674 Iteration 12 X value is 1.2777337898784 Iteration 13 X value is 1.152179114080832 Iteration 14 X value is 1.0291355317992152 Iteration 15 X value is 0.9085528211632309 Iteration 16 X value is 0.7903817647399662 Iteration 17 X value is 0.6745741294451669 Iteration 18 X value is 0.5610826468562635 Iteration 19 X value is 0.44986099391913825 Iteration 20 X value is 0.3408637740407555 Iteration 21 X value is 0.23404649855994042 Iteration 22 X value is 0.1293655685887416 Iteration 23 X value is 0.026778257216966764 Iteration 24 X value is -0.07375730792737258 Iteration 25 X value is -0.1722821617688251 Iteration 26 X value is -0.2688365185334486 Iteration 27 X value is -0.36345978816277963 Iteration 28 X value is -0.45619059239952403 Iteration 29 X value is -0.5470667805515336 Iteration 30 X value is -0.6361254449405029 Iteration 31 X value is -0.7234029360416929 Iteration 32 X value is -0.8089348773208591 Iteration 33 X value is -0.8927561797744419 Iteration 34 X value is -0.9749010561789531 Iteration 35 X value is -1.055403035055374 Iteration 36 X value is -1.1342949743542665 Iteration 37 X value is -1.2116090748671813 Iteration 38 X value is -1.2873768933698377 Iteration 39 X value is -1.361629355502441 Iteration 40 X value is -1.4343967683923922 Iteration 41 X value is -1.5057088330245443 Iteration 42 X value is -1.5755946563640535 Iteration 43 X value is -1.6440827632367725 Iteration 44 X value is -1.711201107972037 Iteration 45 X value is -1.7769770858125964 Iteration 46 X value is -1.8414375440963444 Iteration 47 X value is -1.9046087932144176 Iteration 48 X value is -1.9665166173501292 Iteration 49 X value is -2.0271862850031264 Iteration 50 X value is -2.0866425593030637 Iteration 51 X value is -2.1449097081170025 Iteration 52 X value is -2.2020115139546625 Iteration 53 X value is -2.257971283675569 Iteration 54 X value is -2.312811858002058 Iteration 55 X value is -2.3665556208420164 Iteration 56 X value is -2.419224508425176 Iteration 57 X value is -2.4708400182566725 Iteration 58 X value is -2.521423217891539 Iteration 59 X value is -2.570994753533708 Iteration 60 X value is -2.619574858463034 Iteration 61 X value is -2.667183361293773 Iteration 62 X value is -2.713839694067898 Iteration 63 X value is -2.75956290018654 Iteration 64 X value is -2.804371642182809 Iteration 65 X value is -2.8482842093391527 Iteration 66 X value is -2.8913185251523696 Iteration 67 X value is -2.9334921546493224 Iteration 68 X value is -2.974822311556336 Iteration 69 X value is -3.015325865325209 Iteration 70 X value is -3.055019348018705 Iteration 71 X value is -3.093918961058331 Iteration 72 X value is -3.1320405818371646 Iteration 73 X value is -3.1693997702004215 Iteration 74 X value is -3.206011774796413 Iteration 75 X value is -3.2418915393004846 Iteration 76 X value is -3.277053708514475 Iteration 77 X value is -3.3115126343441856 Iteration 78 X value is -3.345282381657302 Iteration 79 X value is -3.378376734024156 Iteration 80 X value is -3.4108091993436727 Iteration 81 X value is -3.4425930153567994 Iteration 82 X value is -3.4737411550496633 Iteration 83 X value is -3.50426633194867 Iteration 84 X value is -3.534181005309697 Iteration 85 X value is -3.563497385203503 Iteration 86 X value is -3.5922274374994325 Iteration 87 X value is -3.620382888749444 Iteration 88 X value is -3.6479752309744553 Iteration 89 X value is -3.675015726354966 Iteration 90 X value is -3.7015154118278666 Iteration 91 X value is -3.7274851035913095 Iteration 92 X value is -3.7529354015194833 Iteration 93 X value is -3.7778766934890937 Iteration 94 X value is -3.8023191596193118 Iteration 95 X value is -3.8262727764269258 Iteration 96 X value is -3.8497473208983872 Iteration 97 X value is -3.8727523744804193 Iteration 98 X value is -3.895297326990811 Iteration 99 X value is -3.917391380450995 Iteration 100 X value is -3.939043552841975 Iteration 101 X value is -3.9602626817851356 Iteration 102 X value is -3.981057428149433 Iteration 103 X value is -4.001436279586445 Iteration 104 X value is -4.021407553994716 Iteration 105 X value is -4.040979402914822 Iteration 106 X value is -4.060159814856525 Iteration 107 X value is -4.078956618559395 Iteration 108 X value is -4.097377486188207 Iteration 109 X value is -4.115429936464443 Iteration 110 X value is -4.133121337735154 Iteration 111 X value is -4.150458910980451 Iteration 112 X value is -4.167449732760842 Iteration 113 X value is -4.1841007381056246 Iteration 114 X value is -4.200418723343512 Iteration 115 X value is -4.216410348876642 Iteration 116 X value is -4.2320821418991095 Iteration 117 X value is -4.247440499061128 Iteration 118 X value is -4.262491689079905 Iteration 119 X value is -4.277241855298307 Iteration 120 X value is -4.291697018192341 Iteration 121 X value is -4.305863077828494 Iteration 122 X value is -4.319745816271924 Iteration 123 X value is -4.333350899946486 Iteration 124 X value is -4.3466838819475555 Iteration 125 X value is -4.359750204308605 Iteration 126 X value is -4.372555200222433 Iteration 127 X value is -4.385104096217984 Iteration 128 X value is -4.3974020142936245 Iteration 129 X value is -4.409453974007752 Iteration 130 X value is -4.421264894527597 Iteration 131 X value is -4.432839596637045 Iteration 132 X value is -4.444182804704305 Iteration 133 X value is -4.4552991486102185 Iteration 134 X value is -4.466193165638014 Iteration 135 X value is -4.4768693023252535 Iteration 136 X value is -4.487331916278748 Iteration 137 X value is -4.497585277953173 Iteration 138 X value is -4.50763357239411 Iteration 139 X value is -4.517480900946228 Iteration 140 X value is -4.527131282927304 Iteration 141 X value is -4.536588657268758 Iteration 142 X value is -4.545856884123382 Iteration 143 X value is -4.5549397464409145 Iteration 144 X value is -4.563840951512097 Iteration 145 X value is -4.572564132481855 Iteration 146 X value is -4.581112849832218 Iteration 147 X value is -4.589490592835574 Iteration 148 X value is -4.597700780978863 Iteration 149 X value is -4.605746765359285 Iteration 150 X value is -4.6136318300521 Iteration 151 X value is -4.621359193451058 Iteration 152 X value is -4.628932009582036 Iteration 153 X value is -4.636353369390395 Iteration 154 X value is -4.643626302002588 Iteration 155 X value is -4.650753775962536 Iteration 156 X value is -4.657738700443285 Iteration 157 X value is -4.664583926434419 Iteration 158 X value is -4.671292247905731 Iteration 159 X value is -4.6778664029476165 Iteration 160 X value is -4.684309074888664 Iteration 161 X value is -4.6906228933908904 Iteration 162 X value is -4.696810435523073 Iteration 163 X value is -4.702874226812612 Iteration 164 X value is -4.708816742276359 Iteration 165 X value is -4.714640407430832 Iteration 166 X value is -4.720347599282215 Iteration 167 X value is -4.725940647296571 Iteration 168 X value is -4.731421834350639 Iteration 169 X value is -4.736793397663627 Iteration 170 X value is -4.742057529710355 Iteration 171 X value is -4.747216379116147 Iteration 172 X value is -4.752272051533824 Iteration 173 X value is -4.757226610503148 Iteration 174 X value is -4.762082078293084 Iteration 175 X value is -4.766840436727223 Iteration 176 X value is -4.771503627992678 Iteration 177 X value is -4.776073555432824 Iteration 178 X value is -4.780552084324168 Iteration 179 X value is -4.784941042637685 Iteration 180 X value is -4.7892422217849315 Iteration 181 X value is -4.793457377349233 Iteration 182 X value is -4.7975882298022485 Iteration 183 X value is -4.801636465206204 Iteration 184 X value is -4.805603735902079 Iteration 185 X value is -4.809491661184038 Iteration 186 X value is -4.813301827960357 Iteration 187 X value is -4.81703579140115 Iteration 188 X value is -4.820695075573127 Iteration 189 X value is -4.824281174061665 Iteration 190 X value is -4.827795550580431 Iteration 191 X value is -4.831239639568823 Iteration 192 X value is -4.834614846777447 Iteration 193 X value is -4.837922549841898 Iteration 194 X value is -4.84116409884506 Iteration 195 X value is -4.844340816868159 Iteration 196 X value is -4.847454000530796 Iteration 197 X value is -4.85050492052018 Iteration 198 X value is -4.853494822109776 Iteration 199 X value is -4.85642492566758 Iteration 200 X value is -4.859296427154229 Iteration 201 X value is -4.862110498611145 Iteration 202 X value is -4.864868288638922 Iteration 203 X value is -4.867570922866143 Iteration 204 X value is -4.87021950440882 Iteration 205 X value is -4.872815114320644 Iteration 206 X value is -4.875358812034231 Iteration 207 X value is -4.877851635793546 Iteration 208 X value is -4.880294603077676 Iteration 209 X value is -4.882688711016122 Iteration 210 X value is -4.8850349367958 Iteration 211 X value is -4.887334238059884 Iteration 212 X value is -4.8895875532986866 Iteration 213 X value is -4.891795802232712 Iteration 214 X value is -4.893959886188058 Iteration 215 X value is -4.896080688464297 Iteration 216 X value is -4.898159074695011 Iteration 217 X value is -4.9001958932011105 Iteration 218 X value is -4.902191975337089 Iteration 219 X value is -4.904148135830347 Iteration 220 X value is -4.90606517311374 Iteration 221 X value is -4.907943869651465 Iteration 222 X value is -4.909784992258436 Iteration 223 X value is -4.911589292413267 Iteration 224 X value is -4.913357506565002 Iteration 225 X value is -4.915090356433702 Iteration 226 X value is -4.9167885493050285 Iteration 227 X value is -4.918452778318928 Iteration 228 X value is -4.920083722752549 Iteration 229 X value is -4.921682048297498 Iteration 230 X value is -4.923248407331548 Iteration 231 X value is -4.9247834391849175 Iteration 232 X value is -4.926287770401219 Iteration 233 X value is -4.927762014993195 Iteration 234 X value is -4.929206774693331 Iteration 235 X value is -4.930622639199464 Iteration 236 X value is -4.932010186415474 Iteration 237 X value is -4.933369982687164 Iteration 238 X value is -4.934702583033421 Iteration 239 X value is -4.936008531372753 Iteration 240 X value is -4.937288360745298 Iteration 241 X value is -4.938542593530392 Iteration 242 X value is -4.939771741659784 Iteration 243 X value is -4.940976306826588 Iteration 244 X value is -4.942156780690056 Iteration 245 X value is -4.943313645076255 Iteration 246 X value is -4.94444737217473 Iteration 247 X value is -4.945558424731236 Iteration 248 X value is -4.946647256236611 Iteration 249 X value is -4.947714311111879 Iteration 250 X value is -4.9487600248896415 Iteration 251 X value is -4.949784824391848 Iteration 252 X value is -4.950789127904011 Iteration 253 X value is -4.951773345345931 Iteration 254 X value is -4.952737878439012 Iteration 255 X value is -4.953683120870232 Iteration 256 X value is -4.954609458452827 Iteration 257 X value is -4.955517269283771 Iteration 258 X value is -4.956406923898095 Iteration 259 X value is -4.957278785420133 Iteration 260 X value is -4.958133209711731 Iteration 261 X value is -4.958970545517496 Iteration 262 X value is -4.959791134607146 Iteration 263 X value is -4.960595311915003 Iteration 264 X value is -4.9613834056767026 Iteration 265 X value is -4.962155737563169 Iteration 266 X value is -4.962912622811905 Iteration 267 X value is -4.963654370355667 Iteration 268 X value is -4.964381282948554 Iteration 269 X value is -4.965093657289583 Iteration 270 X value is -4.965791784143791 Iteration 271 X value is -4.966475948460915 Iteration 272 X value is -4.967146429491697 Iteration 273 X value is -4.967803500901863 Iteration 274 X value is -4.968447430883826 Iteration 275 X value is -4.969078482266149 Iteration 276 X value is -4.969696912620826 Iteration 277 X value is -4.970302974368409 Iteration 278 X value is -4.970896914881041 Iteration 279 X value is -4.97147897658342 Iteration 280 X value is -4.972049397051752 Iteration 281 X value is -4.972608409110717 Iteration 282 X value is -4.973156240928502 Iteration 283 X value is -4.973693116109932 Iteration 284 X value is -4.974219253787734 Iteration 285 X value is -4.974734868711979 Iteration 286 X value is -4.975240171337739 Iteration 287 X value is -4.975735367910985 Iteration 288 X value is -4.976220660552765 Iteration 289 X value is -4.976696247341709 Iteration 290 X value is -4.977162322394875 Iteration 291 X value is -4.977619075946977 Iteration 292 X value is -4.978066694428038 Iteration 293 X value is -4.978505360539477 Iteration 294 X value is -4.978935253328687 Iteration 295 X value is -4.979356548262113 Iteration 296 X value is -4.979769417296871 Iteration 297 X value is -4.980174028950934 Iteration 298 X value is -4.980570548371915 Iteration 299 X value is -4.980959137404477 Iteration 300 X value is -4.981339954656387 Iteration 301 X value is -4.981713155563259 Iteration 302 X value is -4.982078892451994 Iteration 303 X value is -4.9824373146029535 Iteration 304 X value is -4.982788568310895 Iteration 305 X value is -4.983132796944677 Iteration 306 X value is -4.983470141005784 Iteration 307 X value is -4.983800738185668 Iteration 308 X value is -4.984124723421955 Iteration 309 X value is -4.984442228953515 Iteration 310 X value is -4.984753384374445 Iteration 311 X value is -4.985058316686956 Iteration 312 X value is -4.9853571503532175 Iteration 313 X value is -4.985650007346153 Iteration 314 X value is -4.9859370071992295 Iteration 315 X value is -4.986218267055245 Iteration 316 X value is -4.98649390171414 Iteration 317 X value is -4.986764023679857 Iteration 318 X value is -4.98702874320626 Iteration 319 X value is -4.987288168342134 Iteration 320 X value is -4.987542404975292 Iteration 321 X value is -4.987791556875786 Iteration 322 X value is -4.98803572573827 Iteration 323 X value is -4.988275011223505 Iteration 324 X value is -4.988509510999035 Iteration 325 X value is -4.988739320779054 Iteration 326 X value is -4.988964534363473 Iteration 327 X value is -4.989185243676204 Iteration 328 X value is -4.98940153880268 Iteration 329 X value is -4.989613508026626 Iteration 330 X value is -4.989821237866094 Iteration 331 X value is -4.990024813108772 Iteration 332 X value is -4.9902243168465965 Iteration 333 X value is -4.990419830509665 Iteration 334 X value is -4.990611433899471 Iteration 335 X value is -4.990799205221482 Iteration 336 X value is -4.990983221117052 Iteration 337 X value is -4.991163556694711 Iteration 338 X value is -4.991340285560817 Iteration 339 X value is -4.9915134798496 Iteration 340 X value is -4.991683210252608 Iteration 341 X value is -4.991849546047556 Iteration 342 X value is -4.992012555126605 Iteration 343 X value is -4.992172304024073 Iteration 344 X value is -4.992328857943591 Iteration 345 X value is -4.99248228078472 Iteration 346 X value is -4.992632635169025 Iteration 347 X value is -4.9927799824656445 Iteration 348 X value is -4.992924382816332 Iteration 349 X value is -4.993065895160005 Iteration 350 X value is -4.993204577256805 Iteration 351 X value is -4.993340485711669 Iteration 352 X value is -4.993473675997436 Iteration 353 X value is -4.993604202477487 Iteration 354 X value is -4.993732118427937 Iteration 355 X value is -4.993857476059379 Iteration 356 X value is -4.993980326538191 Iteration 357 X value is -4.9941007200074266 Iteration 358 X value is -4.994218705607278 Iteration 359 X value is -4.994334331495133 Iteration 360 X value is -4.994447644865231 Iteration 361 X value is -4.994558691967926 Iteration 362 X value is -4.994667518128567 Iteration 363 X value is -4.994774167765996 Iteration 364 X value is -4.9948786844106765 Iteration 365 X value is -4.994981110722463 Iteration 366 X value is -4.995081488508014 Iteration 367 X value is -4.995179858737854 Iteration 368 X value is -4.995276261563097 Iteration 369 X value is -4.995370736331835 Iteration 370 X value is -4.9954633216051985 Iteration 371 X value is -4.995554055173095 Iteration 372 X value is -4.995642974069633 Iteration 373 X value is -4.99573011458824 Iteration 374 X value is -4.995815512296476 Iteration 375 X value is -4.995899202050547 Iteration 376 X value is -4.995981218009535 Iteration 377 X value is -4.996061593649345 Iteration 378 X value is -4.996140361776358 Iteration 379 X value is -4.996217554540831 Iteration 380 X value is -4.996293203450014 Iteration 381 X value is -4.996367339381013 Iteration 382 X value is -4.996439992593393 Iteration 383 X value is -4.996511192741525 Iteration 384 X value is -4.996580968886694 Iteration 385 X value is -4.99664934950896 Iteration 386 X value is -4.9967163625187805 Iteration 387 X value is -4.996782035268405 Iteration 388 X value is -4.996846394563037 Iteration 389 X value is -4.996909466671776 Iteration 390 X value is -4.996971277338341 Iteration 391 X value is -4.997031851791574 Iteration 392 X value is -4.997091214755742 Iteration 393 X value is -4.997149390460628 Iteration 394 X value is -4.997206402651415 Iteration 395 X value is -4.997262274598387 Iteration 396 X value is -4.997317029106419 Iteration 397 X value is -4.997370688524291 Iteration 398 X value is -4.997423274753805 Iteration 399 X value is -4.997474809258729 Iteration 400 X value is -4.997525313073554 Iteration 401 X value is -4.997574806812083 Iteration 402 X value is -4.997623310675841 Iteration 403 X value is -4.997670844462324 Iteration 404 X value is -4.997717427573078 Iteration 405 X value is -4.997763079021617 Iteration 406 X value is -4.997807817441185 Iteration 407 X value is -4.997851661092361 Iteration 408 X value is -4.997894627870514 Iteration 409 X value is -4.997936735313104 Iteration 410 X value is -4.9979780006068415 Iteration 411 X value is -4.998018440594705 Iteration 412 X value is -4.998058071782811 Iteration 413 X value is -4.998096910347155 Iteration 414 X value is -4.998134972140212 Iteration 415 X value is -4.998172272697408 Iteration 416 X value is -4.9982088272434595 Iteration 417 X value is -4.998244650698591 Iteration 418 X value is -4.998279757684619 Iteration 419 X value is -4.998314162530927 Iteration 420 X value is -4.998347879280309 Iteration 421 X value is -4.998380921694703 Iteration 422 X value is -4.998413303260809 Iteration 423 X value is -4.998445037195593 Iteration 424 X value is -4.998476136451681 Iteration 425 X value is -4.998506613722648 Iteration 426 X value is -4.998536481448195 Iteration 427 X value is -4.998565751819231 Iteration 428 X value is -4.998594436782846 Iteration 429 X value is -4.998622548047189 Iteration 430 X value is -4.998650097086245 Iteration 431 X value is -4.9986770951445205 Iteration 432 X value is -4.99870355324163 Iteration 433 X value is -4.998729482176797 Iteration 434 X value is -4.998754892533261 Iteration 435 X value is -4.998779794682596 Iteration 436 X value is -4.998804198788944 Iteration 437 X value is -4.998828114813166 Iteration 438 X value is -4.998851552516903 Iteration 439 X value is -4.998874521466565 Iteration 440 X value is -4.998897031037234 Iteration 441 X value is -4.998919090416489 Iteration 442 X value is -4.99894070860816 Iteration 443 X value is -4.998961894435997 Iteration 444 X value is -4.998982656547277 Iteration 445 X value is -4.999003003416331 Iteration 446 X value is -4.999022943348004 Iteration 447 X value is -4.999042484481044 Iteration 448 X value is -4.999061634791423 Iteration 449 X value is -4.999080402095594 Iteration 450 X value is -4.999098794053682 Iteration 451 X value is -4.999116818172609 Iteration 452 X value is -4.999134481809157 Iteration 453 X value is -4.999151792172974 Iteration 454 X value is -4.999168756329515 Iteration 455 X value is -4.999185381202924 Iteration 456 X value is -4.999201673578866 Iteration 457 X value is -4.999217640107289 Iteration 458 X value is -4.999233287305143 Iteration 459 X value is -4.9992486215590395 Iteration 460 X value is -4.999263649127859 Iteration 461 X value is -4.999278376145302 Iteration 462 X value is -4.999292808622396 Iteration 463 X value is -4.999306952449948 Iteration 464 X value is -4.999320813400949 Iteration 465 X value is -4.99933439713293 Iteration 466 X value is -4.999347709190272 Iteration 467 X value is -4.9993607550064665 Iteration 468 X value is -4.999373539906337 Iteration 469 X value is -4.99938606910821 Iteration 470 X value is -4.9993983477260455 Iteration 471 X value is -4.999410380771525 Iteration 472 X value is -4.999422173156094 Iteration 473 X value is -4.9994337296929725 Iteration 474 X value is -4.999445055099113 Iteration 475 X value is -4.999456153997131 Iteration 476 X value is -4.999467030917188 Iteration 477 X value is -4.9994776902988445 Iteration 478 X value is -4.999488136492867 Iteration 479 X value is -4.99949837376301 Iteration 480 X value is -4.99950840628775 Iteration 481 X value is -4.999518238161995 Iteration 482 X value is -4.999527873398756 Iteration 483 X value is -4.99953731593078 Iteration 484 X value is -4.999546569612165 Iteration 485 X value is -4.999555638219921 Iteration 486 X value is -4.999564525455523 Iteration 487 X value is -4.999573234946412 Iteration 488 X value is -4.9995817702474845 Iteration 489 X value is -4.999590134842535 Iteration 490 X value is -4.999598332145684 Iteration 491 X value is -4.99960636550277 Iteration 492 X value is -4.999614238192715 Iteration 493 X value is -4.999621953428861 Iteration 494 X value is -4.999629514360284 Iteration 495 X value is -4.999636924073078 Iteration 496 X value is -4.999644185591617 Iteration 497 X value is -4.999651301879784 Iteration 498 X value is -4.999658275842188 Iteration 499 X value is -4.999665110325345 Iteration 500 X value is -4.999671808118838 Iteration 501 X value is -4.9996783719564615 Iteration 502 X value is -4.999684804517332 Iteration 503 X value is -4.999691108426985 Iteration 504 X value is -4.999697286258446 Iteration 505 X value is -4.9997033405332765 Iteration 506 X value is -4.999709273722611 Iteration 507 X value is -4.999715088248159 Iteration 508 X value is -4.999720786483196 Iteration 509 X value is -4.999726370753532 Iteration 510 X value is -4.999731843338461 Iteration 511 X value is -4.999737206471692 Iteration 512 X value is -4.999742462342258 Iteration 513 X value is -4.999747613095413 Iteration 514 X value is -4.999752660833504 Iteration 515 X value is -4.999757607616834 Iteration 516 X value is -4.999762455464498 Iteration 517 X value is -4.999767206355208 Iteration 518 X value is -4.999771862228104 Iteration 519 X value is -4.999776424983542 Iteration 520 X value is -4.9997808964838715 Iteration 521 X value is -4.999785278554194 Iteration 522 X value is -4.9997895729831106 Iteration 523 X value is -4.999793781523448 Iteration 524 X value is -4.999797905892979 Iteration 525 X value is -4.999801947775119 Iteration 526 X value is -4.999805908819617 Iteration 527 X value is -4.999809790643225 Iteration 528 X value is -4.99981359483036 Iteration 529 X value is -4.999817322933753 Iteration 530 X value is -4.999820976475077 Iteration 531 X value is -4.999824556945576 Iteration 532 X value is -4.999828065806665 Iteration 533 X value is -4.9998315044905315 Iteration 534 X value is -4.999834874400721 Iteration 535 X value is -4.999838176912706 Iteration 536 X value is -4.999841413374452 Iteration 537 X value is -4.999844585106963 Iteration 538 X value is -4.999847693404824 Iteration 539 X value is -4.999850739536727 Iteration 540 X value is -4.999853724745993 Iteration 541 X value is -4.999856650251073 Iteration 542 X value is -4.999859517246051 Iteration 543 X value is -4.99986232690113 Iteration 544 X value is -4.999865080363108 Iteration 545 X value is -4.999867778755846 Iteration 546 X value is -4.999870423180729 Iteration 547 X value is -4.999873014717115 Iteration 548 X value is -4.999875554422772 Iteration 549 X value is -4.999878043334316 Iteration 550 X value is -4.99988048246763 Iteration 551 X value is -4.999882872818278 Iteration 552 X value is -4.999885215361912 Iteration 553 X value is -4.999887511054674 Iteration 554 X value is -4.999889760833581 Iteration 555 X value is -4.999891965616909 Iteration 556 X value is -4.999894126304571 Iteration 557 X value is -4.999896243778479 Iteration 558 X value is -4.999898318902909 Iteration 559 X value is -4.999900352524851 Iteration 560 X value is -4.9999023454743545 Iteration 561 X value is -4.999904298564868 Iteration 562 X value is -4.9999062125935705 Iteration 563 X value is -4.999908088341699 Iteration 564 X value is -4.9999099265748645 Iteration 565 X value is -4.999911728043367 Iteration 566 X value is -4.9999134934825 Iteration 567 X value is -4.99991522361285 Iteration 568 X value is -4.999916919140593 Iteration 569 X value is -4.999918580757781 Iteration 570 X value is -4.999920209142625 Iteration 571 X value is -4.999921804959773 Iteration 572 X value is -4.9999233688605775 Iteration 573 X value is -4.999924901483366 Iteration 574 X value is -4.999926403453699 Iteration 575 X value is -4.999927875384625 Iteration 576 X value is -4.999929317876933 Iteration 577 X value is -4.999930731519394 Iteration 578 X value is -4.999932116889006 Iteration 579 X value is -4.999933474551226 Iteration 580 X value is -4.999934805060202 Iteration 581 X value is -4.999936108958998 Iteration 582 X value is -4.999937386779818 Iteration 583 X value is -4.999938639044221 Iteration 584 X value is -4.999939866263337 Iteration 585 X value is -4.99994106893807 Iteration 586 X value is -4.999942247559309 Iteration 587 X value is -4.999943402608123 Iteration 588 X value is -4.9999445345559606 Iteration 589 X value is -4.999945643864842 Iteration 590 X value is -4.999946730987545 Iteration 591 X value is -4.999947796367794 Iteration 592 X value is -4.999948840440438 Iteration 593 X value is -4.999949863631629 Iteration 594 X value is -4.999950866358997 Iteration 595 X value is -4.9999518490318176 The local minimum occurs at -4.9999518490318176 . Additional Resource Markdown Overview on Bias and Variance Bias Variance Tradeoff End To End Guide For Machine Learning Project Implementing Gradient Descent .",
            "url": "https://entiretydotai.github.io/blogs/jupyter/ml-basics/bias/variance/gradientdescent/2020/03/19/Machine-Learning-Basics.html",
            "relUrl": "/jupyter/ml-basics/bias/variance/gradientdescent/2020/03/19/Machine-Learning-Basics.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://entiretydotai.github.io/blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://entiretydotai.github.io/blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About us",
          "content": ". Entirety.ai is a ML/DL Meetup Community based out of Bengaluru.We are also a part of Pie and AI Meetup Global Community which is run by deeplearning.ai . In our meetup,we discuss current trend in deep-learning and it’s surprisingly effective role in solving real case scenarios. This meetup aims to educate, inspire, and enable you to rapidly prototype your next idea using ML and DL models. The strategy will be to focus more on Hands On experience first, and then, take you deeper into concepts. Artificial Intelligence and Machine Learning are evolving extremely fast which makes the concepts invented last year, obsolete this year. Therefore we will cover mostly the latest concepts used in the industry. This meetup focuses on “how to build and understand”, not just “how to use”. . Find us at: . Meetup . Github . Telegram . Twitter . Mail Us :- entirety.ai@gmail.com .",
          "url": "https://entiretydotai.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}